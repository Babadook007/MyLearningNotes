{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Source : https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"vivek_spark_app\").getOrCreate()\nsdf = spark.read.load(\"../input/bank.csv\",format=\"csv\",inferSchema= True,header=True,sep=\";\")\n# sdf.take(2)\nsdf = sdf.withColumnRenamed(\"y\",\"deposit\")\nsdf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pdf = sdf.toPandas()\npdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(sdf.take(5), columns=sdf.columns).traspose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = [t[0] for t in sdf.dtypes if t[1] == 'int']\nnum_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### https://spark.apache.org/docs/latest/ml-features.html#stringindexer\n### https://spark.apache.org/docs/latest/ml-features.html#onehotencoderestimator\n### https://spark.apache.org/docs/latest/ml-features.html#vectorassembler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\ncategoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\nstages = []\nfor categoricalCol in categoricalColumns:\n    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    stages += [stringIndexer, encoder]\nlabel_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\nstages += [label_stringIdx]\nnumericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nprint(assemblerInputs)\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nprint([assembler])\nstages += [assembler]\nprint(stages)","execution_count":48,"outputs":[{"output_type":"stream","text":"['jobclassVec', 'maritalclassVec', 'educationclassVec', 'defaultclassVec', 'housingclassVec', 'loanclassVec', 'contactclassVec', 'poutcomeclassVec', 'age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n[VectorAssembler_31a1ec005d1f]\n[StringIndexer_85c9ffc6d54e, OneHotEncoderEstimator_350a52e7db43, StringIndexer_293849b93586, OneHotEncoderEstimator_20aa317bce70, StringIndexer_4643093f60b6, OneHotEncoderEstimator_e6ace1af176f, StringIndexer_47c6e89afead, OneHotEncoderEstimator_5b5ea6091e4f, StringIndexer_9ae623a5eb28, OneHotEncoderEstimator_18ae1eb47e41, StringIndexer_1f976458f01c, OneHotEncoderEstimator_f65015e3d2eb, StringIndexer_1a82d8c8935d, OneHotEncoderEstimator_aad460c1002a, StringIndexer_5ce9db597dc4, OneHotEncoderEstimator_753a9c84b04e, StringIndexer_a267d6bcd402, VectorAssembler_31a1ec005d1f]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml import Pipeline\ncols = sdf.columns\npipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(sdf)\nsdf = pipelineModel.transform(sdf)\nselectedCols = ['label', 'features'] + cols\nsdf = sdf.select(selectedCols)\nsdf.printSchema()","execution_count":49,"outputs":[{"output_type":"stream","text":"root\n |-- label: double (nullable = false)\n |-- features: vector (nullable = true)\n |-- age: integer (nullable = true)\n |-- job: string (nullable = true)\n |-- marital: string (nullable = true)\n |-- education: string (nullable = true)\n |-- default: string (nullable = true)\n |-- balance: integer (nullable = true)\n |-- housing: string (nullable = true)\n |-- loan: string (nullable = true)\n |-- contact: string (nullable = true)\n |-- day: integer (nullable = true)\n |-- month: string (nullable = true)\n |-- duration: integer (nullable = true)\n |-- campaign: integer (nullable = true)\n |-- pdays: integer (nullable = true)\n |-- previous: integer (nullable = true)\n |-- poutcome: string (nullable = true)\n |-- deposit: string (nullable = true)\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sdf.select('features').take(2))\nsdf.select('label').take(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = p=sdf.randomSplit([0.7, 0.3], seed = 2018)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth=3) #maxDepth to avoide overfitting\n\ndtModel = dt.fit(train) # train the model\npredictions_dt = dtModel.transform(test) # test the model / make prediction \n\npredictions_dt.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.evaluation import BinaryClassificationEvaluator\neval = BinaryClassificationEvaluator()\neval.evaluate(predictions_dt,{eval.metricName: \"areaUnderROC\"})\n#ROC ?","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label' )\nrfModel = rf.fit(train)\npredictions_rf = rfModel.transform(test)\npredictions_rf.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').toPandas()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval = BinaryClassificationEvaluator()\neval.evaluate(predictions_rf,{eval.metricName: 'areaUnderROC'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient-Boosted Tree Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import GBTClassifier\n\ngbt = GBTClassifier()\ngbtModel = gbt.fit(train)\ngbtPrediction =  gbtModel.transform(test)\ngbtPrediction.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').toPandas()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval.evaluate(gbtPrediction,{eval.metricName: \"areaUnderROC\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}