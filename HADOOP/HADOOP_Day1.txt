Shared Location: \\192.168.1.54\d\Hadoop

The pwd for all pdf files are hdp

House Keeping

10-11.45
15 min break
11.45 to 1.30
45 min break
2.15 - 3.30 ~ 3.45
15 min break
wrap up by 5.45

Pre-Req stuff for the next --> Video's

1) Ensure that your VMware workstation is opening
2) Copy the Spark_Ubuntu14.4_WithVim.zip to your local system AND extract it.

Big Data --> Term for datasets which is so huge that traditional data processing applications cannot do __,__, things.

Hadoop --> Framework for doing 2 things

	a) Storage --> HDFS


	b) Processing --> can be done in 2 ways

Processing --> Batch		v/s 	Streaming [ Apache Storm ]

REPL: Repeate Execute Print Loop

		1) Traditional Map Reduce
		2) Spark --> In memory technique [ scala, python ]		
Lucene --> 

Analysis		v/s		Analytics

Reporting [ historical]			Predictive
RDBMS, BI				RDBMS,NoSQL - SPSS, SAS, R

ETL --> Abnitio, DataStage, Informatica


OLTP			v/s		OLAP

Transactional				Analytical
RDBMS					DWH
NoSQL					Hadoop


Biggest Diffentiator between RDBMS and Hadoop

Centralized 	v/s 	Distributed	--> Storage + Processing


Data Lake Implementation --> Business Term



ETL			v/s		ELT

Extract Transform Load			Extract Load Transform
Process while streaming			Hadoop

Laptop Pre-Req

1) Min 4 GB, recommended 8GB
2) 64 bit processor
3) Enable Intel Virtualization


============================> Dev - Ops technique 

1) Hadoop Version --> Gen2 / Hadoop2 --> 2.7.2 --> YARN [ Yet Another Resource Negotiator ]

2) Virtualization Software --> VmWare Work station

=============================>

Big data --> 3 v's - IBM

Big data is a term for data sets that are so large or complex that traditional data processing applications are inadequate to deal with them.

Challenges include analysis, capture, data curation, search, sharing, storage, transfer, visualization, querying, updating and information privacy.  


1) Volume --> How many zeros in yotta bytes. --> 24 zeros.

2) Velocity --> Speed of the data arrival.

	--> Algo Trading
	--> AML
	--> Fraud Analytics
3) Variety:-

	a) Structured --> schema aware, fixed rows and cols, data types
	b) Semi-Structured --> textual data having no schema -- emails, logs, blogs, comments, JSON
	c) UnStructured --> 
		images -- Cheques 
		video -- CCTV, security
		audio -- trader's call, Customer confirmation calls

http://www.ibmbigdatahub.com/infographic/four-vs-big-data

====================> What is Hadoop

Apache Hadoop 

1) is an open-source software framework 
2) used for distributed storage and distributed processing 
3) of very large data sets. 
4) It consists of computer clusters built from commodity hardware.

For Distributed Storage 	--> HDFS
For Distributed Processing 	--> Map Reduce

================================================================>
Data Processing Speeds

Data: txns - 9 cols
Size: 176 MB
Records: 20,00,000

memory		512 GB			1024 GB		8 GB
processor	8			16		2 core
		RDBMS			DWH		Hadoop


Load		4 - 5 min		1 - 2 min	4 sec

Processing
select col1, col2
from table. 
Non indexed	20 min			5 min		35 - 45 sec

10 million						65 - 75 sec

================================================================>	


Data going to Code		v/s 		Code going to Data
Traditionally					Hadoop
Centralized					Distributed

==================> Post Morning Tea

Load the image [ Customized Ubuntu image, which was created from the base Ubuntu.iso] and power it on

What is Virtualization

1) It is the ability to run a different OS within a OS. [ We will be running ubuntu inside windows ]

What are the 4 common virtualization softwares

1) VmWare
2) Virtual box
3) Hyper-V
4) KVM

==============================>


3 pre-req before powering on any Image

1) memory --> 1/2 of the base memory
2) processor --> change the Number of core per processor -2
3) Network Adapter --> NAT

username: notroot
password: hadoop123

==============================>

1) Copy putty and winscp to your local system
2) Install winscp --> FTP tool plus also allows us to edit files like notepad
3) start winscp and putty to connect to the remote system.

==============================>

Distributions of Hadoop

1) Cloudera
2) Hortonworks
3) Pivotal
4) MapR
5) IBM Big Insights
6) HDInsight by microsoft

---> https://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support


Use Cases of Big Data

http://timesofindia.indiatimes.com/delhi-elections-2015/top-stories/Delhi-elections-2015-How-a-small-IIT-B-team-shaped-AAPs-Delhi-poll-campaign/articleshow/46173477.cms

http://www.infoworld.com/article/2613587/big-data/the-real-story-of-how-big-data-analytics-helped-obama-win.html

https://www.techgig.com/tech-news/editors-pick/this-is-how-trump-is-using-data-analytics-to-win-us-elections-55228

http://www.datasciencecentral.com/profiles/blogs/the-amazing-ways-uber-is-using-big-data

https://wiki.apache.org/hadoop/PoweredBy

http://hadoopilluminated.com/hadoop_illuminated/Hadoop_Use_Cases.html

http://www.cloudera.com/customers.html

===============================================> HDFS Architecture

1) Master - Slave Architecture --> Master [ Server Grade ] & Slave [ Commodity machine ]

Commodity --> No dual power supply, No RAID, No high memory configuration, huge amount of storage.

4PB --> 250 nodes - each node have 6 TB storage.

2) Block size --> Unit of data that can be read / written.
	Linux --> 4K
	Hadoop --> 128MB [ Everything is configurable ]

3) Redundancy:- Block Replication. The default is 3 [ 1+2 ], on different nodes. This can be changed at the file level granurality.

All the modules in Hadoop are designed with a fundamental assumption that hardware failures are a common occurrence and should be automatically handled by the framework.
		
File Injestion Process
Cluster --> set of machines acting like one entity.

Client A: sample - 200 MB -- put this in to the cluster.
[Not a part of the cluster ]

				Master

1	2	3	4	5	6	7	8	9	10
128	72			128	72	72		128

1) Whom will the client talk to? Master
2) How will the client know who the master is? via config files. Should hadoop be present on the client? YES only the extracted hadoop and the config files.
3) What will be the master's reponse to the client request? He will give the write pipeline [ slave ips ]. How many slave ips will be give.

	128[ 1, 5, 9 ]
	 72[ 2, 6, 7 ]

Note: The client will write only to the first node for every block.
4) How is the first node for a block decided by the master? Based on the network proximity between the client and the slaves and then of course the avilability.
5) How are the other nodes decided? NOT BASED ON PROXIMITY NOW. But based on the data distribution [ load balancing ]. If there are multiple nodes available, then based on random distribution.
6) When will replication start? Immediately [at 4 k levels ]when writing happens on the first node for every block. 
7) When will the final write confirmation go from the client to the master? Once the reverse checksum information comes from 9 - 5 - 1 and then from 7 - 6 to 2 and then 1 and 2 will give it to the client.

===========================================> File Read Architecture

					Master

1	2	3	4	5	6	7	8	9	10
128	72			128	72	72		128

							Client B
						Read Pipeline
						[9,5,1]
						[7,6,2]


==================> Post Lunch
1) created the dir structure in ubuntu /home/notroot
2) extracted hadoop
3) configured the .bashrc and ran it
4) checked the javac and hadoop versions

=============================================>

Failure Use Cases :-

 1) While Writing
	a) Primary Node[1,2]: Node 1 goes down after 100 MB of data is written.

	1) Will the client be aware of the failure? YES. As he has opened socket connection to the node 1 and 2 for writing.
	2) What should the client do now? Start the write process again. Again communicate back to the master, who will send a new write pipeline.
	3) How much data should the client send now as a part of the write process? The complete file has to be rewritten.
	4) What will happen to the 100MB & 72 MB data written and replicated. --> Orphaned or zombied blocks, which needs to be deleted via a cron job, which runs fsck.This has to be executed explicitly.

				Master

1	2	3	4	5	6	7	8	9	10
128	72			128	72	72		128

	b) Replicated Nodes: Node 9 goes down after 100 MB of data is written.
	1) Will the client be aware of the failure? No
	2) How will the framework handle it. The master will assign a new node for the failed node. [ 8 ], and will inform 5 to write the complete block to 8.

Is HDFS a physical or a virtual file system? Virtual.
If we put a file called sample into the cluster.

	a) Where will i see the file sample? HDFS level
	b) Will i see the file sample at the node level? No, we will only see the blk files.

 2) After Writing

	Any node goes down. Will the Admin have to do anything other then formatting the failed node? Nothing.

	The master [ hadoop framework ]  will assign a different node in place of the failed node and request a node which has the data to copy this on to the newly assigned node. The frame work will ensure that at any point of time, it adheres to the replication policy.

	--> What will happen is the failed node comes -->

	Less than 10 minutes --> It will automatically added back to the cluster.

	More than 10 minutes --> all the blocks in the node is invalid.

=================================================>

Master --> metadata
Slaves --> actual data

1) install npp
2) extract hadoop2.7.2 in windows

Configure the XML file -->

core --> common to both hdfs and YARN
mapred
hdfs
yarn - processing component of hadoop

For each of the components, there is a xml file.

There would be a XXX-default.xml file which will give you all the listing of the default properties.

There will be a XXX-site.xml file inside etc/hadoop folder of the hadoop installation, where we will have to write the properties which needs to be overridden.

Let us go ahead and change the xml files --> 

https://www.youtube.com/watch?v=hsq4s_l9ZDM --> Data Lake Architecture

=================================> Post Evening Tea

deamons in hadoop --> 

			HDFS			YARN

Master			Namenode		ResourceManager

Slave			Datanode		NodeManager

			SecondaryNameNode OR
			Passive Namenode

4 node cluster --> on which machine will the deamons be running

	Master	{	NN	SNN	RM	} --> 3 machines

	Slaves	{	DN+NM	DN+NM	DN+NM	DN+NM	}


http://media.bestofmicro.com/X/8/430172/original/yarn.png

	
			NN	SNN	RM	Slave	Client

core-site.xml
	fs.defaultFS	YES	YES	YES	YES	YES

hdfs-site.xml
	dfs.replication	YES	No	No	No	No
dfs.namenode.name.dir	YES	No	No	No	No
dfs.datanode.data.dir	No	No	No	YES	No

mapred-site.xml		No	No	YES	No	No
yarn-site.xml		No	No	YES	No	No

=======================================================>

By now you would have had your 5 deamons up and running -->

Finish off till step no 20.

=========================>

NN Http no is: 50070
RM Http no is: 8088
Job History Server Http no: 19888

==============================> How metadata is handled in HDFS

For the first time, when we format the NN and start the services

1) fsimage --> snapshot of the FS at a point of time
2) edits_in_progress
3) edits_XXXXXX03-XXXXXXXX40

	--> HDFS Architecture with respect to Metadata

https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html

https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Secondary_NameNode

Where is the meta data of the cluster stored?

1) Main memory of the NN		PLUS
2) a persisted copy will be present in fsimage and edits

Pre-Req for Day2

1) https://www.youtube.com/watch?v=bcjSe0xCHbE
2) https://www.youtube.com/watch?v=ht3dNvdNDzI

Checkpointing process

1) The SNN will copy the fsimage and edits in progress to its local system, load it in to memory, apply the edits in progress and create a new fsimage

4 clock --> fsimage_20 edits in progress
5 clock --> SNN will copy these files, apply the image and get a new fsimage_30
6 clock --> SNN will copy these files, apply the image and get a new fsimage_40
6.30 --> restart my machine --> 


2) It will then copy the new fsimage to the directory in the NN also.

===============================> HDFS steps from 20 to 26

1) stop-all.sh
2) stop the job history server
3) close winscp
4) close putty
5) power off the Image --> VM Menu in the workstation - Power - Power off

The folder Spark_Ubuntu14.4 contains all the work you have done today.


The location of the metadata will be present in /tmp/hadoop-notroot/namesecondary folder.

	



